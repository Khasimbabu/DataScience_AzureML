Work with Data in Azure Machine Learning
*******************************************
===============================================

Data is the foundation of machine learning. In this module, you will learn how to work with datastores and datasets in Azure Machine Learning, enabling you to build scalable, cloud-based model training solutions.

how to create and manage datastores and datasets in an Azure Machine Learning workspace, and how to use them in model training experiments.

In Azure Machine Learning, datastores are abstractions for cloud data sources. They encapsulate the information required to connect to data sources. You can access datastores directly in code by using the Azure Machine Learning SDK, and use it to upload or download data.

Types of Datastore
Azure Machine Learning supports the creation of datastores for multiple kinds of Azure data source, including:

Azure Storage (blob and file containers)
Azure Data Lake stores
Azure SQL Database
Azure Databricks file system (DBFS)

Every workspace has two built-in datastores (an Azure Storage blob container, and an Azure Storage file container) that are used as system storage by Azure Machine Learning. 

Use datastores
********************
To add a datastore to your workspace, you can register it using the graphical interface in Azure Machine Learning studio, or you can use the Azure Machine Learning SDK. For example, the following code registers an Azure Storage blob container as a datastore named blob_data.

>>>>>>>>>>>>>>>>>
from azureml.core import Workspace, Datastore

ws = Workspace.from_config()

# Register a new datastore
blob_ds = Datastore.register_azure_blob_container(workspace=ws, 
                                                  datastore_name='blob_data', 
                                                  container_name='data_container',
                                                  account_name='az_store_acct',
                                                  account_key='123456abcde789â€¦')
>>>>>>>>>>>>

Managing datastores
************************
The following code lists the names of each datastore in the workspace:
for ds_name in ws.datastores:
    print(ds_name)

You can get a reference to any datastore by using the Datastore.get() method as shown here:
blob_store = Datastore.get(ws, datastore_name='blob_data')

The workspace always includes a default datastore (initially, this is the built-in workspaceblobstore datastore), which you can retrieve by using the get_default_datastore() method of a Workspace object, like this:
default_store = ws.get_default_datastore()

To change the default datastore, use the set_default_datastore() method:
ws.set_default_datastore('blob_data')

Datasets
*************
Datasets are versioned packaged data objects that can be easily consumed in experiments and pipelines. Datasets are the recommended way to work with data, and are the primary mechanism for advanced Azure Machine Learning capabilities like data labeling and data drift monitoring.

Datasets are typically based on files in a datastore, though they can also be based on URLs and other sources. You can create the following types of dataset:

Tabular - You should use this type of dataset when your data is consistently structured 
File - when your data is unstructured (for example, to train a convolutional neural network from a set of image files).

After you've created a dataset, you can register it in the workspace to make it available for use in experiments and data processing pipelines later.

Creating and registering tabular datasets
*******************************************
To create a tabular dataset using the SDK, use the from_delimited_files method of the Dataset.Tabular class, like this:

>>>>>>>>
from azureml.core import Dataset

blob_ds = ws.get_default_datastore()
csv_paths = [(blob_ds, 'data/files/current_data.csv'),
             (blob_ds, 'data/files/archive/*.csv')]
tab_ds = Dataset.Tabular.from_delimited_files(path=csv_paths)
tab_ds = tab_ds.register(workspace=ws, name='csv_table')
>>>>>>>>>>>

The current_data.csv file in the data/files folder.
All .csv files in the data/files/archive/ folder.

After creating the dataset, the code registers it in the workspace with the name csv_table.

Creating and registering file datasets
**********************************************
To create a file dataset using the SDK, use the from_files method of the Dataset.File class, like this:

>>>>>>>>>>>
from azureml.core import Dataset

blob_ds = ws.get_default_datastore()
file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))
file_ds = file_ds.register(workspace=ws, name='img_files')
>>>>>>>>>>>>>>

The dataset in this example includes all .jpg files in the data/files/images path within the default datastore:

After creating the dataset, the code registers it in the workspace with the name img_files.

Retrieving a registered dataset
***********************************
After registering a dataset, you can retrieve it by using any of the following techniques:

The datasets dictionary attribute of a Workspace object.
The get_by_name or get_by_id method of the Dataset class.

Both of these techniques are shown in the following code:
>>>>>>>>>>>>>>>>
import azureml.core
from azureml.core import Workspace, Dataset

# Load the workspace from the saved config file
ws = Workspace.from_config()

# Get a dataset from the workspace datasets collection
ds1 = ws.datasets['csv_table']

# Get a dataset by name from the datasets class
ds2 = Dataset.get_by_name(ws, 'img_files')
>>>>>>>>>>>>>>>>>>>

Dataset versioning
**********************
Datasets can be versioned, enabling you to track historical versions of datasets that were used in experiments, and reproduce those experiments with data in the same state.

You can create a new version of a dataset by registering it with the same name as a previously registered dataset and specifying the create_new_version property:
>>>>>>>>>>
img_paths = [(blob_ds, 'data/files/images/*.jpg'),
             (blob_ds, 'data/files/images/*.png')]
file_ds = Dataset.File.from_files(path=img_paths)
file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)
>>>>>>>>>>>>>>

Retrieving a specific dataset version
****************************************
img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)

Use datasets
****************
Datasets are the primary way to pass data to experiments that train models.

Work with tabular datasets
*****************************
You can read data directly from a tabular dataset by converting it into a Pandas or Spark dataframe:

df = tab_ds.to_pandas_dataframe()
# code to work with dataframe goes here, for example:
print(df.head())

Pass a tabular dataset to an experiment script
************************************************
1) Use a script argument for a tabular dataset

ScriptRunConfig:
--------------------
env = Environment('my_env')
packages = CondaDependencies.create(conda_packages=['pip'],
                                    pip_packages=['azureml-defaults',
                                                  'azureml-dataprep[pandas]'])
env.python.conda_dependencies = packages

script_config = ScriptRunConfig(source_directory='my_dir',
                                script='script.py',
                                arguments=['--ds', tab_ds],
                                environment=env)
>>>>>>>>>>>>>>>>>>
from azureml.core import Run, Dataset

parser.add_argument('--ds', type=str, dest='dataset_id')
args = parser.parse_args()

run = Run.get_context()
ws = run.experiment.workspace
dataset = Dataset.get_by_id(ws, id=args.dataset_id)
data = dataset.to_pandas_dataframe()
>>>>>>>>>>>>>>>>>

2) Use a named input for a tabular dataset
ScriptRunConfig:
------------------
env = Environment('my_env')
packages = CondaDependencies.create(conda_packages=['pip'],
                                    pip_packages=['azureml-defaults',
                                                  'azureml-dataprep[pandas]'])
env.python.conda_dependencies = packages

script_config = ScriptRunConfig(source_directory='my_dir',
                                script='script.py',
                                arguments=['--ds', tab_ds.as_named_input('my_dataset')],
                                environment=env)
>>>>>>>>>>>>>>>>>>>>>>

from azureml.core import Run

parser.add_argument('--ds', type=str, dest='ds_id')
args = parser.parse_args()

run = Run.get_context()
dataset = run.input_datasets['my_dataset']
data = dataset.to_pandas_dataframe()
>>>>>>>>>>>>>>>>>

Work with file datasets
***************************
When working with a file dataset, you can use the to_path() method to return a list of the file paths encapsulated by the dataset:

for file_path in file_ds.to_path():
    print(file_path)

Use a script argument for a file dataset
*******************************************
You can pass a file dataset as a script argument. Unlike with a tabular dataset, you must specify a mode for the file dataset argument, which can be as_download or as_mount. 

ScriptRunConfig:
-------------------
env = Environment('my_env')
packages = CondaDependencies.create(conda_packages=['pip'],
                                    pip_packages=['azureml-defaults',
                                                  'azureml-dataprep[pandas]'])
env.python.conda_dependencies = packages

script_config = ScriptRunConfig(source_directory='my_dir',
                                script='script.py',
                                arguments=['--ds', file_ds.as_download()],
                                environment=env)
>>>>>>>>>>>>>>>>>>>>>>>>>>>
from azureml.core import Run
import glob

parser.add_argument('--ds', type=str, dest='ds_ref')
args = parser.parse_args()
run = Run.get_context()

imgs = glob.glob(ds_ref + "/*.jpg")
>>>>>>>>>>>>>>>

Use a named input for a file dataset
***************************************
You can also pass a file dataset as a named input. In this approach, you use the as_named_input method of the dataset to specify a name before specifying the access mode.

ScriptRunConfig:
-------------------
env = Environment('my_env')
packages = CondaDependencies.create(conda_packages=['pip'],
                                    pip_packages=['azureml-defaults',
                                                  'azureml-dataprep[pandas]'])
env.python.conda_dependencies = packages

script_config = ScriptRunConfig(source_directory='my_dir',
                                script='script.py',
                                arguments=['--ds', file_ds.as_named_input('my_ds').as_download()],
                                environment=env)
>>>>>>>>>>>>>>>>
from azureml.core import Run
import glob

parser.add_argument('--ds', type=str, dest='ds_ref')
args = parser.parse_args()
run = Run.get_context()

dataset = run.input_datasets['my_ds']
imgs= glob.glob(dataset + "/*.jpg")
>>>>>>>>>>>>>>>>>

In this exercise, you will:

Upload data to a datastore
Create datasets
Use datasets to train a model

















