Work with Compute in Azure Machine Learning
*******************************************
===============================================
One of the key benefits of the cloud is the ability to use scalable, on-demand compute resources for cost-effective processing of large data. In this module, you'll learn how to use cloud compute in Azure Machine Learning to run training experiments at scale.

In Azure Machine Learning, data scientists can run experiments based on scripts that process data, train machine learning models, and perform other data science tasks. The runtime context for each experiment run consists of two elements:

The environment for the script, which includes all packages on which the script depends.
The compute target on which the environment will be deployed and the script run.

Python code runs in the context of a virtual environment that defines the version of the Python runtime to be used as well as the installed packages available to the code. In most Python installations, packages are installed and managed in environments using Conda or pip.

To improve portability, we usually create environments in docker containers that are in turn be hosted in compute targets, such as your development computer, virtual machines, or clusters in the cloud.

Environments in Azure Machine Learning
*********************************************
In general, Azure Machine Learning handles environment creation and package installation for you - usually through the creation of Docker containers. You can specify the Conda or pip packages you need, and have Azure Machine Learning create an environment for the experiment.

In an enterprise machine learning solution, where experiments may be run in a variety of compute contexts, it can be important to be aware of the environments in which your experiment code is running. Environments are encapsulated by the Environment class; which you can use to create environments and specify runtime configuration for an experiment.

You can have Azure Machine Learning manage environment creation and package installation to define an environment, and then register it for reuse. Alternatively, you can manage your own environments and register them. This makes it possible to define consistent, reusable runtime contexts for your experiments - regardless of where the experiment script is run.

Creating an environment from a specification file
******************************************************
You can use a Conda or pip specification file to define the packages required in a Python environment, and use it to create an Environment object.

>>>>>>>>>>>
name: py_env
dependencies:
  - numpy
  - pandas
  - scikit-learn
  - pip:
    - azureml-defaults
>>>>>>>>>>>>

You could then use the following code to create an Azure Machine Learning environment from the saved specification file:

from azureml.core import Environment
env = Environment.from_conda_specification(name='training_environment',
                                           file_path='./conda.yml')

Creating an environment from an existing Conda environment
************************************************************
If you have an existing Conda environment defined on your workstation, you can use it to define an Azure Machine Learning environment:

from azureml.core import Environment
env = Environment.from_existing_conda_environment(name='training_environment',
                                                  conda_environment_name='py_env')

Creating an environment by specifying packages
**************************************************
You can define an environment by specifying the Conda and pip packages you need in a CondaDependencies object, like this:

from azureml.core import Environment
from azureml.core.conda_dependencies import CondaDependencies

env = Environment('training_environment')
deps = CondaDependencies.create(conda_packages=['scikit-learn','pandas','numpy'],
                                pip_packages=['azureml-defaults'])
env.python.conda_dependencies = deps

Configuring environment containers
*************************************
Usually, you should create environments in containers (this is the default unless the docker.enabled property is set to False, in which case the environment is created directly in the compute target)

env.docker.enabled = True
deps = CondaDependencies.create(conda_packages=['scikit-learn','pandas','pip'],                      
                                pip_packages=['azureml-defaults'])
env.python.conda_dependencies = deps

Azure Machine Learning uses a library of base images for containers, choosing the appropriate base for the compute target you specify (for example, including Cuda support for GPU-based compute). If you have created custom container images and registered them in a container registry, you can override the default base images and use your own.

env.docker.base_image='my-base-image'
env.docker.base_image_registry='myregistry.azurecr.io/myimage'

Alternatively, you can have an image created on-demand based on the base image and additional settings in a dockerfile.

env.docker.base_image = None
env.docker.base_dockerfile = './Dockerfile'

By default, Azure machine Learning handles Python paths and package dependencies. If your image already includes an installation of Python with the dependencies you need, you can override this behavior by setting python.user_managed_dependencies to True and setting an explicit Python path for your installation.

env.python.user_managed_dependencies=True
env.python.interpreter_path = '/opt/miniconda/bin/python'

Registering and reusing environments
********************************************
After you've created an environment, you can register it in your workspace and reuse it for future experiments that have the same Python dependencies.

Use the register method of an Environment object to register an environment:

env.register(workspace=ws)

You can view the registered environments in your workspace like this:

from azureml.core import Environment
env_names = Environment.list(workspace=ws)
for env_name in env_names:
    print('Name:',env_name)

Retrieving and using an environment
********************************************
You can retrieve a registered environment by using the get method of the Environment class, and then assign it to a ScriptRunConfig.

For example, the following code sample retrieves the training_environment registered environment, and assigns it to a script run configuration:

from azureml.core import Environment, ScriptRunConfig
training_env = Environment.get(workspace=ws, name='training_environment')
script_config = ScriptRunConfig(source_directory='my_dir',
                                script='script.py',
                                environment=training_env)

When an experiment based on the estimator is run, Azure Machine Learning will look for an existing environment that matches the definition, and if none is found a new environment will be created based on the registered environment specification.

Introduction to compute targets
***********************************
In Azure Machine Learning, Compute Targets are physical or virtual computers on which experiments are run.

Types of compute
******************
Azure Machine Learning supports multiple types of compute for experimentation and training. This enables you to select the most appropriate type of compute target for your particular needs.

Local compute - You can specify a local compute target for most processing tasks in Azure Machine Learning. This runs the experiment on the same compute target as the code used to initiate the experiment, which may be your physical workstation or a virtual machine such as an Azure Machine Learning compute instance on which you are running a notebook. Local compute is generally a great choice during development and testing with low to moderate volumes of data.

Compute clusters - For experiment workloads with high scalability requirements, you can use Azure Machine Learning compute clusters; which are multi-node clusters of Virtual Machines that automatically scale up or down to meet demand. This is a cost-effective way to run experiments that need to handle large volumes of data or use parallel processing to distribute the workload and reduce the time it takes to run.

Attached compute - If you already use an Azure-based compute environment for data science, such as a virtual machine or an Azure Databricks cluster, you can attach it to your Azure Machine Learning workspace and use it as a compute target for certain types of workload.

In Azure Machine Learning studio, you can create another type of compute named inference clusters. This kind of compute represents an Azure Kubernetes Service cluster and can only be used to deploy trained models as inferencing services. We'll explore deployment later, but for now we'll focus on compute for experiments and model training.

One of the core benefits of cloud computing is the ability to manage costs by paying only for what you use. In Azure Machine Learning, you can take advantage of this principle by defining compute targets that:

Start on-demand and stop automatically when no longer required.
Scale automatically based on workload processing needs.

Creating a managed compute target with the SDK
****************************************************
The most common ways to create or attach a compute target are to use the Compute page in Azure Machine Learning studio, or to use the Azure Machine Learning SDK to provision compute targets in code.

A managed compute target is one that is managed by Azure Machine Learning, such as an Azure Machine Learning compute cluster.

To create an Azure Machine Learning compute cluster, use the azureml.core.compute.ComputeTarget class and the AmlCompute class, like this:

>>>>>>>>>>>>>>>
from azureml.core import Workspace
from azureml.core.compute import ComputeTarget, AmlCompute

# Load the workspace from the saved config file
ws = Workspace.from_config()

# Specify a name for the compute (unique within the workspace)
compute_name = 'aml-cluster'

# Define compute configuration
compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',
                                                       min_nodes=0, max_nodes=4,
                                                       vm_priority='dedicated')

# Create the compute
aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)
aml_cluster.wait_for_completion(show_output=True)
>>>>>>>>>>>>>>>>>>>>>>>>>

In this example, a cluster with up to four nodes that is based on the STANDARD_DS12_v2 virtual machine image will be created. The priority for the virtual machines (VMs) is set to dedicated, meaning they are reserved for use in this cluster (the alternative is to specify lowpriority, which has a lower cost but means that the VMs can be preempted if a higher-priority workload requires the compute).

Attaching an unmanaged compute target with the SDK
**********************************************************
An unmanaged compute target is one that is defined and managed outside of the Azure Machine Learning workspace; for example, an Azure virtual machine or an Azure Databricks cluster.

The code to attach an existing unmanaged compute target is similar to the code used to create a managed compute target, except that you must use the ComputeTarget.attach() method to attach the existing compute based on its target-specific configuration settings.

For example, the following code can be used to attach an existing Azure Databricks cluster:

>>>>>>>>>>>>>>>
from azureml.core import Workspace
from azureml.core.compute import ComputeTarget, DatabricksCompute

# Load the workspace from the saved config file
ws = Workspace.from_config()

# Specify a name for the compute (unique within the workspace)
compute_name = 'db_cluster'

# Define configuration for existing Azure Databricks cluster
db_workspace_name = 'db_workspace'
db_resource_group = 'db_resource_group'
db_access_token = '1234-abc-5678-defg-90...'
db_config = DatabricksCompute.attach_configuration(resource_group=db_resource_group,
                                                   workspace_name=db_workspace_name,
                                                   access_token=db_access_token)

# Create the compute
databricks_compute = ComputeTarget.attach(ws, compute_name, db_config)
databricks_compute.wait_for_completion(True)
>>>>>>>>>>>>>>>

Checking for an existing compute target
********************************************
In many cases, you will want to check for the existence of a compute target, and only create a new one if there isn't already one with the specified name. To accomplish this, you can catch the ComputeTargetException exception, like this:

from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.core.compute_target import ComputeTargetException

compute_name = "aml-cluster"

# Check if the compute target exists
try:
    aml_cluster = ComputeTarget(workspace=ws, name=compute_name)
    print('Found existing cluster.')
except ComputeTargetException:
    # If not, create it
    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2',
                                                           max_nodes=4)
    aml_cluster = ComputeTarget.create(ws, compute_name, compute_config)

aml_cluster.wait_for_completion(show_output=True)

>>>>>>>>>>>>>>>>>>>>>>>>>

Use compute targets
******************************
After you've created or attached compute targets in your workspace, you can use them to run specific workloads; such as experiments.

To use a particular compute target, you can specify it in the appropriate parameter for an experiment run configuration or estimator. For example, the following code configures an estimator to use the compute target named aml-cluster:

>>>>>>>>>>>>
from azureml.core import Environment, ScriptRunConfig

compute_name = 'aml-cluster'

training_env = Environment.get(workspace=ws, name='training_environment')

script_config = ScriptRunConfig(source_directory='my_dir',
                                script='script.py',
                                environment=training_env,
                                compute_target=compute_name)
>>>>>>>>>>>>>>>>>>

When an experiment is submitted, the run will be queued while the aml-cluster compute target is started and the specified environment created on it, and then the run will be processed on the compute environment.

Instead of specifying the name of the compute target, you can specify a ComputeTarget object, like this:
>>>>>>>>>>>>>>>>>
from azureml.core import Environment, ScriptRunConfig
from azureml.core.compute import ComputeTarget

compute_name = "aml-cluster"

training_cluster = ComputeTarget(workspace=ws, name=compute_name)

training_env = Environment.get(workspace=ws, name='training_environment')

script_config = ScriptRunConfig(source_directory='my_dir',
                                script='script.py',
                                environment=training_env,
                                compute_target=training_cluster)
>>>>>>>>>>>>>>>>>>>>>>>>


























